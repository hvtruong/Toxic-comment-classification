This is a project replicating the Toxic comment classification challenge: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

In this project, we train a LSTM model to take Wikipedia comments and classify them as one more more labels: toxic, severe toxic, obscene, insult, threat, identitiy hate based on probabilities.
The classification generated by our model resulted a ROC-AUC score of 0.97621.

All the data and code is vailable at: https://drive.google.com/drive/u/1/folders/1bhdyVjjGFXiRDHngFqb2-RIgzkhaKs5y